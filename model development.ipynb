{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5fb32624-0307-4bbe-a6d7-9deb8210f348",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-30 23:37:04,110 - INFO - Training and tuning: Logistic Regression\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['background_color', 'most_used_font_face', 'secure_protocol'], dtype='object')\n",
      "Index(['page_load_time', 'image_percentage', 'video_percentage',\n",
      "       'text_percentage', 'internal_links', 'external_links'],\n",
      "      dtype='object')\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-30 23:37:06,417 - ERROR - Error training Logistic Regression: Invalid parameter 'logisticregression' for estimator Pipeline(steps=[('preprocessor',\n",
      "                 ColumnTransformer(transformers=[('cat', OneHotEncoder(),\n",
      "                                                  Index(['background_color', 'most_used_font_face', 'secure_protocol'], dtype='object')),\n",
      "                                                 ('num', StandardScaler(),\n",
      "                                                  Index(['page_load_time', 'image_percentage', 'video_percentage',\n",
      "       'text_percentage', 'internal_links', 'external_links'],\n",
      "      dtype='object'))])),\n",
      "                ('classifier', LogisticRegression(class_weight='balanced'))]). Valid parameters are: ['memory', 'steps', 'verbose'].\n",
      "2024-09-30 23:37:06,418 - INFO - Training and tuning: SVM\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-30 23:37:08,636 - ERROR - Error training SVM: Invalid parameter 'svc' for estimator Pipeline(steps=[('preprocessor',\n",
      "                 ColumnTransformer(transformers=[('cat', OneHotEncoder(),\n",
      "                                                  Index(['background_color', 'most_used_font_face', 'secure_protocol'], dtype='object')),\n",
      "                                                 ('num', StandardScaler(),\n",
      "                                                  Index(['page_load_time', 'image_percentage', 'video_percentage',\n",
      "       'text_percentage', 'internal_links', 'external_links'],\n",
      "      dtype='object'))])),\n",
      "                ('classifier', SVC(class_weight='balanced', probability=True))]). Valid parameters are: ['memory', 'steps', 'verbose'].\n",
      "2024-09-30 23:37:08,641 - INFO - Training and tuning: Random Forest\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-30 23:37:11,030 - ERROR - Error training Random Forest: Invalid parameter 'randomforestclassifier' for estimator Pipeline(steps=[('preprocessor',\n",
      "                 ColumnTransformer(transformers=[('cat', OneHotEncoder(),\n",
      "                                                  Index(['background_color', 'most_used_font_face', 'secure_protocol'], dtype='object')),\n",
      "                                                 ('num', StandardScaler(),\n",
      "                                                  Index(['page_load_time', 'image_percentage', 'video_percentage',\n",
      "       'text_percentage', 'internal_links', 'external_links'],\n",
      "      dtype='object'))])),\n",
      "                ('classifier', RandomForestClassifier(random_state=42))]). Valid parameters are: ['memory', 'steps', 'verbose'].\n",
      "2024-09-30 23:37:11,033 - INFO - Training and tuning: Gradient Boosting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-30 23:37:13,433 - ERROR - Error training Gradient Boosting: Invalid parameter 'gradientboostingclassifier' for estimator Pipeline(steps=[('preprocessor',\n",
      "                 ColumnTransformer(transformers=[('cat', OneHotEncoder(),\n",
      "                                                  Index(['background_color', 'most_used_font_face', 'secure_protocol'], dtype='object')),\n",
      "                                                 ('num', StandardScaler(),\n",
      "                                                  Index(['page_load_time', 'image_percentage', 'video_percentage',\n",
      "       'text_percentage', 'internal_links', 'external_links'],\n",
      "      dtype='object'))])),\n",
      "                ('classifier', GradientBoostingClassifier())]). Valid parameters are: ['memory', 'steps', 'verbose'].\n",
      "2024-09-30 23:37:13,434 - INFO - Training and tuning: K-Nearest Neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-30 23:37:15,762 - ERROR - Error training K-Nearest Neighbors: Invalid parameter 'kneighborsclassifier' for estimator Pipeline(steps=[('preprocessor',\n",
      "                 ColumnTransformer(transformers=[('cat', OneHotEncoder(),\n",
      "                                                  Index(['background_color', 'most_used_font_face', 'secure_protocol'], dtype='object')),\n",
      "                                                 ('num', StandardScaler(),\n",
      "                                                  Index(['page_load_time', 'image_percentage', 'video_percentage',\n",
      "       'text_percentage', 'internal_links', 'external_links'],\n",
      "      dtype='object'))])),\n",
      "                ('classifier', KNeighborsClassifier())]). Valid parameters are: ['memory', 'steps', 'verbose'].\n",
      "2024-09-30 23:37:15,763 - INFO - Training and tuning: AdaBoost\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-30 23:37:18,143 - ERROR - Error training AdaBoost: Invalid parameter 'adaboostclassifier' for estimator Pipeline(steps=[('preprocessor',\n",
      "                 ColumnTransformer(transformers=[('cat', OneHotEncoder(),\n",
      "                                                  Index(['background_color', 'most_used_font_face', 'secure_protocol'], dtype='object')),\n",
      "                                                 ('num', StandardScaler(),\n",
      "                                                  Index(['page_load_time', 'image_percentage', 'video_percentage',\n",
      "       'text_percentage', 'internal_links', 'external_links'],\n",
      "      dtype='object'))])),\n",
      "                ('classifier', AdaBoostClassifier())]). Valid parameters are: ['memory', 'steps', 'verbose'].\n",
      "2024-09-30 23:37:18,147 - INFO - Training and tuning: Extra Trees\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-30 23:37:20,654 - ERROR - Error training Extra Trees: Invalid parameter 'extratreesclassifier' for estimator Pipeline(steps=[('preprocessor',\n",
      "                 ColumnTransformer(transformers=[('cat', OneHotEncoder(),\n",
      "                                                  Index(['background_color', 'most_used_font_face', 'secure_protocol'], dtype='object')),\n",
      "                                                 ('num', StandardScaler(),\n",
      "                                                  Index(['page_load_time', 'image_percentage', 'video_percentage',\n",
      "       'text_percentage', 'internal_links', 'external_links'],\n",
      "      dtype='object'))])),\n",
      "                ('classifier', ExtraTreesClassifier(random_state=42))]). Valid parameters are: ['memory', 'steps', 'verbose'].\n",
      "2024-09-30 23:37:20,655 - INFO - Training and tuning: XGBoost\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-30 23:37:22,963 - ERROR - Error training XGBoost: Invalid parameter 'xgbclassifier' for estimator Pipeline(steps=[('preprocessor',\n",
      "                 ColumnTransformer(transformers=[('cat', OneHotEncoder(),\n",
      "                                                  Index(['background_color', 'most_used_font_face', 'secure_protocol'], dtype='object')),\n",
      "                                                 ('num', StandardScaler(),\n",
      "                                                  Index(['page_load_time', 'image_percentage', 'video_percentage',\n",
      "       'text_percentage', 'internal_links', 'external_links'],\n",
      "      dtype='object'))])),\n",
      "                ('classifier',\n",
      "                 XGBCla...\n",
      "                               feature_types=None, gamma=None, grow_policy=None,\n",
      "                               importance_type=None,\n",
      "                               interaction_constraints=None, learning_rate=None,\n",
      "                               max_bin=None, max_cat_threshold=None,\n",
      "                               max_cat_to_onehot=None, max_delta_step=None,\n",
      "                               max_depth=None, max_leaves=None,\n",
      "                               min_child_weight=None, missing=nan,\n",
      "                               monotone_constraints=None, multi_strategy=None,\n",
      "                               n_estimators=None, n_jobs=None,\n",
      "                               num_parallel_tree=None, random_state=42, ...))]). Valid parameters are: ['memory', 'steps', 'verbose'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report, precision_recall_curve\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV,learning_curve\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Load your data\n",
    "data = pd.read_csv('cleaned.csv')\n",
    "\n",
    "# Define features and target\n",
    "columns_to_exclude = ['indexing_status', 'label', 'url', 'ssl_issuer', 'ssl_subject', 'ssl_not_before', 'ssl_not_after', 'domain']\n",
    "X = data.drop(columns=columns_to_exclude)\n",
    "y = data['indexing_status']\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "numerical_cols = X.select_dtypes(include=['number']).columns\n",
    "print(categorical_cols)\n",
    "print(numerical_cols)\n",
    "\n",
    "\n",
    "# Encode categorical features using OneHotEncoder\n",
    "# Define the preprocessor with OneHotEncoder for categorical columns and StandardScaler for numerical columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(), categorical_cols),\n",
    "        ('num', StandardScaler(), numerical_cols)\n",
    "    ])\n",
    "\n",
    "# Fit and transform the preprocessor on the feature set X\n",
    "X_transformed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Display the transformed data shape\n",
    "X_transformed.shape\n",
    "\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Logistic Regression': {\n",
    "        'model': LogisticRegression(),\n",
    "        'params': {'classifier__C': [0.1, 1, 10]}\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'model': RandomForestClassifier(random_state=42),\n",
    "        'params': {'classifier__n_estimators': [50, 100, 200]}\n",
    "    },\n",
    "    'SVM': {\n",
    "        'model': SVC(probability=True),\n",
    "        'params': {'classifier__C': [0.1, 1, 10], 'classifier__kernel': ['linear', 'rbf']}\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'model': GradientBoostingClassifier(),\n",
    "        'params': {\n",
    "            'classifier__n_estimators': [50, 100, 200],\n",
    "            'classifier__learning_rate': [0.01, 0.1, 1],\n",
    "            'classifier__max_depth': [3, 5, 7]\n",
    "        }\n",
    "    },\n",
    "    'K-Nearest Neighbors': {\n",
    "        'model': KNeighborsClassifier(),\n",
    "        'params': {'classifier__n_neighbors': [3, 5, 7]}\n",
    "    },\n",
    "    'AdaBoost': {\n",
    "        'model': AdaBoostClassifier(),\n",
    "        'params': {'classifier__n_estimators': [50, 100, 200]}\n",
    "    },\n",
    "    'Extra Trees': {\n",
    "        'model': ExtraTreesClassifier(random_state=42),\n",
    "        'params': {\n",
    "            'classifier__n_estimators': [50, 100, 200],\n",
    "            'classifier__max_depth': [3, 5, 7]\n",
    "        }\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'model': xgb.XGBClassifier(random_state=42),\n",
    "        'params': {\n",
    "            'classifier__n_estimators': [50, 100, 200],\n",
    "            'classifier__learning_rate': [0.01, 0.1, 0.2],\n",
    "            'classifier__max_depth': [3, 5, 7]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function for training and tuning models\n",
    "def train_and_tune_model(name, model, params):\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', LogisticRegression())])\n",
    "    grid_search = GridSearchCV(pipeline, params, cv=5,\n",
    "    scoring='accuracy', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    return grid_search.best_estimator_, grid_search.best_params_\n",
    "\n",
    "# Train and tune models\n",
    "best_models = {}\n",
    "for name, model_info in models.items():\n",
    "    model, params = model_info['model'], model_info['params']\n",
    "    best_model, best_params = train_and_tune_model(name, model, params)\n",
    "    best_models[name] = best_model\n",
    "\n",
    "# Prediction\n",
    "y_preds = {}\n",
    "for name, model in best_models.items():\n",
    "    y_preds[name] = model.predict(X_test)\n",
    "\n",
    "# Error Plots\n",
    "plt.figure(figsize=(12, 6))\n",
    "for name, y_pred in y_preds.items():\n",
    "    error = np.abs(y_test - y_pred)\n",
    "    plt.plot(error, label=name)\n",
    "    plt.title('Error Plot')\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Absolute Error')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ROC Curves\n",
    "plt.figure(figsize=(12, 6))\n",
    "for name, model in best_models.items():\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f'{name} (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.title('Receiver Operating Characteristic (ROC)')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Precision-Recall Curves\n",
    "plt.figure(figsize=(12, 6))\n",
    "for name, model in best_models.items():\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
    "    plt.plot(recall, precision, label=name)\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Learning Curves\n",
    "def plot_learning_curve(model, X, y):\n",
    "    train_sizes, train_scores, test_scores = learning_curve(model, X, y, cv=5, n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10))\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    test_mean = np.mean(test_scores, axis=1)\n",
    "    test_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_sizes, train_mean, 'o-', color='r', label='Training score')\n",
    "    plt.plot(train_sizes, test_mean, 'o-', color='g', label='Crossvalidation score')\n",
    "    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='r')\n",
    "    plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1, color='g')\n",
    "    plt.xlabel('Training Set Size')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title(f'Learning Curve for {model.__class__.__name__}')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot learning curves for each model\n",
    "for name, model in best_models.items():\n",
    "    plot_learning_curve(model, X_train, y_train)\n",
    "\n",
    "# Feature Correlation Matrix\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(X.corr(), annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "    plt.title('Feature Correlation Matrix')\n",
    "    plt.show()\n",
    "\n",
    "# Calibration Curves\n",
    "def plot_calibration_curve(model, X, y):\n",
    "    y_prob = model.predict_proba(X)[:, 1]\n",
    "    fraction_of_positives, mean_predicted_value = calibration_curve(y, y_prob, n_bins=10, strategy='uniform')\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(mean_predicted_value, fraction_of_positives, 's-', label=f'{model.__class__.__name__}')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Perfectly calibrated')\n",
    "    plt.xlabel('Mean Predicted Value')\n",
    "    plt.ylabel('Fraction of Positives')\n",
    "    plt.title(f'Calibration Curve for {model.__class__.__name__}')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Plot calibration curves for each model\n",
    "for name, model in best_models.items():\n",
    "    plot_calibration_curve(model, X_test, y_test)\n",
    "\n",
    "# Feature Importances\n",
    "def plot_feature_importances(model, X):\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.title(f'Feature Importances for {model.__class__.__name__}')\n",
    "    plt.bar(range(X.shape[1]), importances[indices], align='center')\n",
    "    plt.xticks(range(X.shape[1]), X.columns[indices], rotation=90)\n",
    "    plt.xlim([-1, X.shape[1]])\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "# Plot feature importances for each model\n",
    "for name, model in best_models.items():\n",
    "    plot_feature_importances(model, X_train)\n",
    "\n",
    "# Pairplot\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    data_with_target = X.copy()\n",
    "    data_with_target['target'] = y\n",
    "    sns.pairplot(data_with_target, hue='target', diag_kind='kde')\n",
    "    plt.suptitle('Pairplot of Features', y=1.02)\n",
    "    plt.show()\n",
    "    \n",
    "# Confusion Matrix and Classification Reports\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    for i, (name, model) in enumerate(best_models.items()):\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "    plt.subplot(3, 3, i+1)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "    xticklabels=['Class 0', 'Class 1'],\n",
    "    yticklabels=['Class 0', 'Class 1'])\n",
    "    plt.title(f'{name} Confusion Matrix')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Print Classification Reports\n",
    "for name, model in best_models.items():\n",
    "    print(f\"Classification Report for {name}:\\n\")\n",
    "    print(classification_report(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014ebfc4-9f65-4a13-a869-52d75ea61a72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
